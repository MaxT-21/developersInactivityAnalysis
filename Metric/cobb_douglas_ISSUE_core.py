# -*- coding: utf-8 -*-
"""Copia di cobb-douglas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FlKQpEsBAOH9lmi5DsmDgiKS5w_7QQEw

This code will first load your dataset into a Pandas DataFrame. Then, it will create a Cobb-Douglas production function and fit it to the data. Finally, it will print the results of the regression.

To run this code, you will need to install the following Python packages:

*   pandas
*   numpy
*   statsmodels

Once you have installed these packages, you can run the code by saving it as a Jupyter notebook and then clicking the "Run" button.

The output of the code will show you the coefficients of the Cobb-Douglas production function. These coefficients can be interpreted as follows:

* The coefficient of a is the productivity of labor. This is the amount of LOC that is contributed per day by each developer.
* The coefficient of b is the productivity of capital. This is the amount of LOC that is contributed per day by each unit of capital, such as a computer or a server.
"""

import pandas as pd
import numpy as np
import statsmodels.api as sm
from IPython.display import display

# Mount the Google Drive
#from google.colab import drive
#drive.mount('/content/drive')

# Load the dataset from Google Drive
# dev_breaks_count represents how many Core devs were inactive or gone on observation day
data_path = '/content/drive/MyDrive/datasets/oss-productivity/LOC_values_core.csv'
# dev_breaks_count represents how many Core devs were gone on observation day
#data_path = '/content/drive/MyDrive/datasets/oss-productivity/LOC_values_core_gone.csv'
# dev_breaks_count represents how many Truck Factors devs were inactive or gone on observation day
#data_path = '/content/drive/MyDrive/datasets/oss-productivity/LOC_values_TF.csv'
# dev_breaks_count represents how many Truck Factors devs were gone on observation day
#data_path = '/content/drive/MyDrive/datasets/oss-productivity/LOC_values_TF_gone.csv'
data = pd.read_csv(data_path)
# Remove NaN
data = data.dropna()

# Cobb-Douglas productivity function
# Y = A * (L^α) * (K^β)
# Where:
#
# Y represents the output or productivity.
# A is a constant factor representing total factor productivity or the overall efficiency of the production process.
# L represents the labor input.
# K represents the capital input.
# α and β are the respective output elasticities or the shares of labor and capital in the production function.

# Convert "LOC" column to numeric data type
data["LOC"] = pd.to_numeric(data["LOC"])

# Take the logarithm of variables
data['log_LOC'] = np.log1p(data['LOC'])
data['log_project_size'] = np.log1p(data['project_size'])
data['log_project_stars'] = np.log1p(data['project_stars'])
data['log_project_contributors'] = np.log1p(data['project_contributors'])
data['log_project_age'] = np.log1p(data['project_age'])
#convert "project_name" and "project_language" to dummy variables
data = pd.get_dummies(data, columns=['project_name'], drop_first=True)
data = pd.get_dummies(data, columns=['project_language'], drop_first=True)
# Add constant factor representing total factor productivity or the overall efficiency of the production process
data['A_TFP'] = 1

# Define the dependent variable (Y) and the independent variables (X)
Y = data['log_LOC']
X = data[['dev_breaks_count', 'log_project_size', 'log_project_stars',
          'log_project_contributors', 'log_project_age']]

# Select columns that start with dummy variables 'project_language_*' and 'project_name_*'
project_language_cols = data.filter(like='project_language_')
project_name_cols = data.filter(like='project_name_')
# Concatenate the filtered columns with the other the independent variables (X)
X = pd.concat([X, project_language_cols, project_name_cols], axis=1)

# Add a constant column to the independent variables
# The sm.add_constant(X) function from the statsmodels library adds a column of
# ones to the independent variables matrix X, representing the constant term.
# It ensures that the regression model includes an intercept in the equation:
# ln(Y)=ln(A)+α⋅ln(L)+β⋅ln(K)+γ
# Here, γ represents the constant term, and it allows the regression line to be
# shifted vertically in the log-space, capturing the baseline level of ln(Y)
# when all the independent variables are zero.
# By adding the constant term to the independent variables matrix, we ensure
# that the linear regression model estimates the intercept as well, providing a
# complete equation to describe the relationship between the log-transformed
# dependent variable and the log-transformed independent variables.
X = sm.add_constant(X)

# Fit the regression model to estimate the Cobb-Douglas production function
# coefficients
model = sm.OLS(Y, X)
results = model.fit()

# Print the regression results
print(results.summary())

"""Check the OLS assumption on data distribution."""

import matplotlib.pyplot as plt
import scipy.stats as stats
import statsmodels

# Residuals
residuals = results.resid

# Diagnostic plots
fig, ax = plt.subplots(2, 2, figsize=(10, 8))
ax[0, 0].scatter(results.fittedvalues, residuals)
ax[0, 0].set_xlabel('Fitted values')
ax[0, 0].set_ylabel('Residuals')
ax[0, 0].set_title('Residuals vs Fitted')

sm.qqplot(residuals, line='s', ax=ax[0, 1])
ax[0, 1].set_title('QQ Plot')

ax[1, 0].scatter(results.fittedvalues, residuals**2)
ax[1, 0].set_xlabel('Fitted values')
ax[1, 0].set_ylabel('Squared residuals')
ax[1, 0].set_title('Squared residuals vs Fitted')

stats.probplot(residuals, dist="norm", plot=ax[1, 1])
ax[1, 1].set_title('Normal Probability Plot')

plt.tight_layout()
plt.show()

# Breusch-Pagan test for homoscedasticity
_, pvalue, _, _ = statsmodels.stats.diagnostic.het_breuschpagan(results.resid, results.model.exog)
print('Breusch-Pagan test p-value:', pvalue)